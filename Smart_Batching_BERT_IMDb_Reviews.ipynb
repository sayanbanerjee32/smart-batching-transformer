{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Smart Batching Tutorial - IMDb Reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayanbanerjee32/smart-batching-transformer/blob/main/Smart_Batching_BERT_IMDb_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e  \n",
        "https://mccormickml.com/2020/07/29/smart-batching-tutorial/"
      ],
      "metadata": {
        "id": "EbLhnz2e3oYb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9PJns-a7srE"
      },
      "source": [
        "## Install transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XeR8Sbz0B5x"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dannu15yD_tu"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNhRtWPXH9C3"
      },
      "source": [
        "Helper function for formatting elapsed times as `hh:mm:ss`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3dCqjK_9v4q"
      },
      "source": [
        "# Retrieve Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cairkv4V7_IE"
      },
      "source": [
        "## Download Dataset Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx3aeVLJBuKC"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Download dataset.\n",
        "train_data = tfds.load(name=\"imdb_reviews/plain_text\", split=\"train\")\n",
        "test_data = tfds.load(name=\"imdb_reviews/plain_text\", split=\"test\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itjbAgmxEp-F"
      },
      "source": [
        "Convert to python types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2_hJjzDYur"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train_text = []\n",
        "train_labels = []\n",
        "\n",
        "# Loop over the training set...\n",
        "for ex in train_data.as_numpy_iterator():\n",
        "\n",
        "    # The text is a `bytes` object, decode to string.\n",
        "    train_text.append(ex['text'].decode())\n",
        "    train_labels.append(int(ex['label']))\n",
        "\n",
        "test_text = []\n",
        "test_labels = []\n",
        "\n",
        "# Loop over the test set...\n",
        "for ex in test_data.as_numpy_iterator():\n",
        "\n",
        "    # The text is a `bytes` object, decode to string.\n",
        "    test_text.append(ex['text'].decode())\n",
        "    test_labels.append(int(ex['label']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydHIQ_yc9ifE"
      },
      "source": [
        "# Smart Batching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuahmt8IGHvG"
      },
      "source": [
        "## Load Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axa8wTWmGJeG"
      },
      "source": [
        "We'll use the uncased version of BERT-base. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z474sSC6oe7A"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
        "                                          do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Wvtz56l1TR"
      },
      "source": [
        "## Tokenize Without Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngo68TDgJtoJ"
      },
      "source": [
        "max_len = 400"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4Lr7ziRl3Uh"
      },
      "source": [
        "**Tokenize, but don't pad**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unZgL-2VmQFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a8244f-f0ae-425f-97ca-ffc5067098c4"
      },
      "source": [
        "full_input_ids = []\n",
        "labels = []\n",
        "\n",
        "# Tokenize all training examples\n",
        "print('Tokenizing {:,} training samples...'.format(len(train_text)))\n",
        "\n",
        "# Choose an interval on which to print progress updates.\n",
        "update_interval = 5000\n",
        "# good_update_interval(total_iters=len(train_text), num_desired_updates=10)\n",
        "\n",
        "# For each training example...\n",
        "for text in train_text:\n",
        "    \n",
        "    # Report progress.\n",
        "    if ((len(full_input_ids) % update_interval) == 0):\n",
        "        print(f'  Tokenized {len(full_input_ids)} samples.')\n",
        "\n",
        "    # Tokenize the sentence.\n",
        "    input_ids = tokenizer.encode(text=text,           # Movie review text\n",
        "                                 add_special_tokens=True, # Do add specials.\n",
        "                                 max_length=max_len,  # Do truncate to `max_len`\n",
        "                                 truncation=True,     # Do truncate!\n",
        "                                 padding=False)       # Don't pad!\n",
        "                                 \n",
        "    # Add the tokenized result to our list.\n",
        "    full_input_ids.append(input_ids)\n",
        "    \n",
        "print('{:>10,} samples'.format(len(full_input_ids)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing 25,000 training samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 5000 samples.\n",
            "  Tokenized 10000 samples.\n",
            "  Tokenized 15000 samples.\n",
            "  Tokenized 20000 samples.\n",
            "    25,000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-tcq0AUuUgn"
      },
      "source": [
        "## Sort by length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBesjNYe_pVB"
      },
      "source": [
        "# Sort the two lists together by the length of the input sequence.\n",
        "train_samples = sorted(zip(full_input_ids, train_labels), key=lambda x: len(x[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jemdbaV_EHSm"
      },
      "source": [
        "`train_samples` is now a list of tuples of (input_ids, label):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwuwisoREQ0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7be2c40-1496-4211-cfea-1d80533a4e4f"
      },
      "source": [
        "train_samples[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([101, 2023, 3185, 2003, 6659, 2021, 2009, 2038, 2070, 2204, 3896, 1012, 102],\n",
              " 0)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hiemov-psBaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d103ac-ea1e-4fe1-a6fa-2e974e177592"
      },
      "source": [
        "print('Shortest sample:', len(train_samples[0][0]))\n",
        "print('Longest sample:', len(train_samples[-1][0]))\n",
        "max_token_len = len(train_samples[-1][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shortest sample: 13\n",
            "Longest sample: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm0GOKKcuZEl"
      },
      "source": [
        "## Random Batch Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ2SJre0w3mz"
      },
      "source": [
        "Choose our batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPy0XKC8xAWa"
      },
      "source": [
        "batch_size = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlDrZ7RUupqz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0545d5c5-35b9-4dd8-cc58-f032d53fe633"
      },
      "source": [
        "import random\n",
        "\n",
        "# List of batches that we'll construct.\n",
        "batch_ordered_sentences = []\n",
        "batch_ordered_labels = []\n",
        "\n",
        "print(f'Creating training batches of size {batch_size}')\n",
        "\n",
        "# Loop over all of the input samples...    \n",
        "while len(train_samples) > 0:\n",
        "    \n",
        "    # Report progress.\n",
        "    if ((len(batch_ordered_sentences) % 500) == 0):\n",
        "        print(f'  Selected {len(batch_ordered_sentences)} batches.')\n",
        "\n",
        "    # `to_take` is our actual batch size. It will be `batch_size` until \n",
        "    # we get to the last batch, which may be smaller. \n",
        "    to_take = min(batch_size, len(train_samples))\n",
        "\n",
        "    # Pick a random index in the list of remaining samples to start\n",
        "    # our batch at.\n",
        "    select = random.randint(0, len(train_samples) - to_take)\n",
        "\n",
        "    # Select a contiguous batch of samples starting at `select`.\n",
        "    batch = train_samples[select:(select + to_take)]\n",
        "\n",
        "    # Each sample is a tuple--split them apart to create a separate list of \n",
        "    # sequences and a list of labels for this batch.\n",
        "    batch_ordered_sentences.append([s[0] for s in batch])\n",
        "    batch_ordered_labels.append([s[1] for s in batch])\n",
        "\n",
        "    # Remove these samples from the list.\n",
        "    del train_samples[select:select + to_take]\n",
        "\n",
        "print(f'\\n  DONE - {len(batch_ordered_sentences)} batches.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating training batches of size 16\n",
            "  Selected 0 batches.\n",
            "  Selected 500 batches.\n",
            "  Selected 1000 batches.\n",
            "  Selected 1500 batches.\n",
            "\n",
            "  DONE - 1563 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6AwCaAvxXdN"
      },
      "source": [
        "## Add Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9sufUYt7GQA"
      },
      "source": [
        "Add **padding** and create **attention masks** here, and cast everything to **PyTorch tensors** in preparation for our fine-tuning step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCNYTKpixPMJ"
      },
      "source": [
        "import torch\n",
        "\n",
        "py_inputs = []\n",
        "py_attn_masks = []\n",
        "py_labels = []\n",
        "\n",
        "# For each batch...\n",
        "for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n",
        "\n",
        "    # New version of the batch, this time with padded sequences and now with\n",
        "    # attention masks defined.\n",
        "    batch_padded_inputs = []\n",
        "    batch_attn_masks = []\n",
        "    \n",
        "    # First, find the longest sample in the batch. \n",
        "    # Note that the sequences do currently include the special tokens!\n",
        "    max_size = max([len(sen) for sen in batch_inputs])\n",
        "\n",
        "\n",
        "    # For each input in this batch...\n",
        "    for sen in batch_inputs:\n",
        "        \n",
        "        # How many pad tokens do we need to add?\n",
        "        num_pads = max_size - len(sen)\n",
        "\n",
        "        # Add `num_pads` padding tokens to the end of the sequence.\n",
        "        padded_input = sen + [tokenizer.pad_token_id]*num_pads\n",
        "\n",
        "        # Define the attention mask--it's just a `1` for every real token\n",
        "        # and a `0` for every padding token.\n",
        "        attn_mask = [1] * len(sen) + [0] * num_pads\n",
        "\n",
        "        # Add the padded results to the batch.\n",
        "        batch_padded_inputs.append(padded_input)\n",
        "        batch_attn_masks.append(attn_mask)\n",
        "\n",
        "    # Our batch has been padded, so we need to save this updated batch.\n",
        "    # We also need the inputs to be PyTorch tensors, so we'll do that here.\n",
        "    py_inputs.append(torch.tensor(batch_padded_inputs))\n",
        "    py_attn_masks.append(torch.tensor(batch_attn_masks))\n",
        "    py_labels.append(torch.tensor(batch_labels))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TACAmCf-Lavx"
      },
      "source": [
        "calculate the total number of tokens in the training data after using smart batching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPQgl1wYLa59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc7247b-33f5-4067-baab-47ab679625d1"
      },
      "source": [
        "# Get the new list of lengths after sorting.\n",
        "padded_lengths = []\n",
        "\n",
        "# For each batch...\n",
        "for batch in py_inputs:\n",
        "    \n",
        "    # For each sample...\n",
        "    for s in batch:\n",
        "    \n",
        "        # Record its length.\n",
        "        padded_lengths.append(len(s))\n",
        "\n",
        "# Sum up the lengths to the get the total number of tokens after smart batching.\n",
        "smart_token_count = np.sum(padded_lengths)\n",
        "\n",
        "# To get the total number of tokens in the dataset using fixed padding, it's\n",
        "# as simple as the number of samples times our `max_len` parameter (that we\n",
        "# would pad everything to).\n",
        "#fixed_token_count = len(train_text) * max_len\n",
        "fixed_token_count = len(train_text) * max_token_len\n",
        "\n",
        "# Calculate the percentage reduction.\n",
        "prcnt_reduced = (fixed_token_count - smart_token_count) / float(fixed_token_count) \n",
        "\n",
        "print('Total tokens:')\n",
        "print('   Fixed Padding: {:,}'.format(fixed_token_count))\n",
        "print('  Smart Batching: {:,}  ({:.1%} less)'.format(smart_token_count, prcnt_reduced))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens:\n",
            "   Fixed Padding: 10,000,000\n",
            "  Smart Batching: 6,381,984  (36.2% less)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaIlkeK2eDJw"
      },
      "source": [
        "## Old Approach - Fixed Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEwaEqwf6zQW"
      },
      "source": [
        "To see how BERT does on the benchmark *without* smart batching, run the following cell instead. Set the following `use_fixed_padding` to True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCHBwwFxeH-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a00dc73-0c3f-4e97-a888-9d2c2d6845f7"
      },
      "source": [
        "# flip this switch to see effect on training time for fixed padding strategy\n",
        "use_fixed_padding = False\n",
        "\n",
        "# Specify batch_size and truncation length.    \n",
        "def get_fixed_padding_dataset(text_data, labels_data,\n",
        "    batch_size = 16,\n",
        "    max_len = 400):\n",
        "    # Tokenize all training examples\n",
        "    print('Tokenizing {:,} data samples...'.format(len(text_data)))\n",
        "\n",
        "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "    batches_input_ids = []\n",
        "    batches_attention_masks = []\n",
        "    batches_labels = []\n",
        "\n",
        "    update_interval = batch_size * 150 \n",
        "\n",
        "    # For every sentence...\n",
        "    for i in range(0, len(text_data), batch_size):\n",
        "\n",
        "        # Report progress.\n",
        "        if ((i % update_interval) == 0):\n",
        "            print('  Tokenized {:,} samples.'.format(i))\n",
        "\n",
        "        # `encode_plus` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Prepend the `[CLS]` token to the start.\n",
        "        #   (3) Append the `[SEP]` token to the end.\n",
        "        #   (4) Map tokens to their IDs.\n",
        "        #   (5) Pad or truncate the sentence to `max_length`\n",
        "        #   (6) Create attention masks for [PAD] tokens.\n",
        "        encoded_dict = tokenizer.batch_encode_plus(\n",
        "                            text_data[i:i+batch_size], # Batch of sentences to encode.\n",
        "                            add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
        "                            max_length = 400,           # Pad & truncate all sentences.\n",
        "                            padding = 'max_length',     # Pad all to the `max_length` parameter.\n",
        "                            truncation = True,\n",
        "                            return_attention_mask = True,   # Construct attn. masks.\n",
        "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "        \n",
        "        # Add the encoded sentence to the list.    \n",
        "        batches_input_ids.append(encoded_dict['input_ids'])\n",
        "        \n",
        "        # And its attention mask (simply differentiates padding from non-padding).\n",
        "        batches_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        # Add the labels for the batch\n",
        "        batches_labels.append(torch.tensor(labels_data[i:i+batch_size]))\n",
        "    return batches_input_ids, batches_attention_masks, batches_labels\n",
        "\n",
        "if use_fixed_padding:\n",
        "  # Rename the final variable to match the rest of the code in this Notebook.\n",
        "  py_inputs, py_attn_masks, py_labels = get_fixed_padding_dataset(train_text, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing 25,000 training samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 2,400 samples.\n",
            "  Tokenized 4,800 samples.\n",
            "  Tokenized 7,200 samples.\n",
            "  Tokenized 9,600 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "  Tokenized 14,400 samples.\n",
            "  Tokenized 16,800 samples.\n",
            "  Tokenized 19,200 samples.\n",
            "  Tokenized 21,600 samples.\n",
            "  Tokenized 24,000 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvuYLP7TDpXH"
      },
      "source": [
        "# Fine-Tune BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwdfO8VuxHE-"
      },
      "source": [
        "## Load Pre-Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GZPd5vWKrpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a85697-13ee-4471-e4fe-2a17d3276182"
      },
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "# Load the Config object, with an output configured for classification.\n",
        "config = AutoConfig.from_pretrained(pretrained_model_name_or_path='bert-base-uncased',\n",
        "                                    num_labels=2)\n",
        "\n",
        "print('Config type:', str(type(config)), '\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config type: <class 'transformers.models.bert.configuration_bert.BertConfig'> \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNGJIqqVKvZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ba885b-5cc3-4bcf-cb68-80fceb9f8c9e"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Load the pre-trained model for classification, passing in the `config` from\n",
        "# above.\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path='bert-base-uncased',\n",
        "    config=config)\n",
        "\n",
        "print('\\nModel type:', str(type(model)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model type: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa7tgw2-Pttq"
      },
      "source": [
        "Connect to the GPU and load our model onto it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asqmh0uTKuTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6858de4-eea3-4590-c04c-4e66884d2450"
      },
      "source": [
        "import torch\n",
        "\n",
        "print('\\nLoading model to GPU...')\n",
        "device = torch.device('cuda')\n",
        "\n",
        "print('  GPU:', torch.cuda.get_device_name(0))\n",
        "desc = model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading model to GPU...\n",
            "  GPU: Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWT-D4U_Pvx"
      },
      "source": [
        "## Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o-VEBobKwHk"
      },
      "source": [
        "Optimizer and learning rate scheduler for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fccf1c55-dcb6-43d9-9812-9f2de87662d7"
      },
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # This is the value Michael used.\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. I chose to train for 1 simply because the training\n",
        "# time is long. More epochs may improve the model's accuracy.\n",
        "epochs = 1\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# Note that it's the number of *batches*, not *samples*!\n",
        "total_steps = len(py_inputs) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfmWwUR_Sox"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLJ5NCUFQujZ"
      },
      "source": [
        "### `make_smart_batches`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVTpC6V8E2_q"
      },
      "source": [
        "This function combines all of the steps from the \"Smart Batching\" section into a single (re-usable) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5LVFY7MPqRf"
      },
      "source": [
        "def make_smart_batches(text_samples, labels, batch_size):\n",
        "    '''\n",
        "    This function combines all of the required steps to prepare batches.\n",
        "    '''\n",
        "\n",
        "    print('Creating Smart Batches from {:,} examples with batch size {:,}...\\n'.format(len(text_samples), batch_size))\n",
        "\n",
        "    # =========================\n",
        "    #   Tokenize & Truncate\n",
        "    # =========================\n",
        "\n",
        "    full_input_ids = []\n",
        "\n",
        "    # Tokenize all training examples\n",
        "    print('Tokenizing {:,} samples...'.format(len(labels)))\n",
        "\n",
        "    # Choose an interval on which to print progress updates.\n",
        "    update_interval = 5000\n",
        "    # good_update_interval(total_iters=len(labels), num_desired_updates=10)\n",
        "\n",
        "    # For each training example...\n",
        "    for text in text_samples:\n",
        "        \n",
        "        # Report progress.\n",
        "        if ((len(full_input_ids) % update_interval) == 0):\n",
        "            print('  Tokenized {:,} samples.'.format(len(full_input_ids)))\n",
        "\n",
        "        # Tokenize the sample.\n",
        "        input_ids = tokenizer.encode(text=text,              # Text to encode.\n",
        "                                    add_special_tokens=True, # Do add specials.\n",
        "                                    max_length=max_len,      # Do Truncate!\n",
        "                                    truncation=True,         # Do Truncate!\n",
        "                                    padding=False)           # DO NOT pad.\n",
        "                                    \n",
        "        # Add the tokenized result to our list.\n",
        "        full_input_ids.append(input_ids)\n",
        "        \n",
        "    print('DONE.')\n",
        "    print('{:>10,} samples\\n'.format(len(full_input_ids)))\n",
        "\n",
        "    # =========================\n",
        "    #      Select Batches\n",
        "    # =========================    \n",
        "\n",
        "    # Sort the two lists together by the length of the input sequence.\n",
        "    samples = sorted(zip(full_input_ids, labels), key=lambda x: len(x[0]))\n",
        "\n",
        "    print('{:>10,} samples after sorting\\n'.format(len(samples)))\n",
        "\n",
        "    import random\n",
        "\n",
        "    # List of batches that we'll construct.\n",
        "    batch_ordered_sentences = []\n",
        "    batch_ordered_labels = []\n",
        "\n",
        "    print('Creating batches of size {:}...'.format(batch_size))\n",
        "\n",
        "    # Choose an interval on which to print progress updates.\n",
        "    update_interval = 5000\n",
        "    # good_update_interval(total_iters=len(samples), num_desired_updates=10)\n",
        "    \n",
        "    # Loop over all of the input samples...    \n",
        "    while len(samples) > 0:\n",
        "        \n",
        "        # Report progress.\n",
        "        if ((len(batch_ordered_sentences) % update_interval) == 0 \\\n",
        "            and not len(batch_ordered_sentences) == 0):\n",
        "            print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n",
        "\n",
        "        # `to_take` is our actual batch size. It will be `batch_size` until \n",
        "        # we get to the last batch, which may be smaller. \n",
        "        to_take = min(batch_size, len(samples))\n",
        "\n",
        "        # Pick a random index in the list of remaining samples to start\n",
        "        # our batch at.\n",
        "        select = random.randint(0, len(samples) - to_take)\n",
        "\n",
        "        # Select a contiguous batch of samples starting at `select`.\n",
        "        #print(\"Selecting batch from {:} to {:}\".format(select, select+to_take))\n",
        "        batch = samples[select:(select + to_take)]\n",
        "\n",
        "        #print(\"Batch length:\", len(batch))\n",
        "\n",
        "        # Each sample is a tuple--split them apart to create a separate list of \n",
        "        # sequences and a list of labels for this batch.\n",
        "        batch_ordered_sentences.append([s[0] for s in batch])\n",
        "        batch_ordered_labels.append([s[1] for s in batch])\n",
        "\n",
        "        # Remove these samples from the list.\n",
        "        del samples[select:select + to_take]\n",
        "\n",
        "    print('\\n  DONE - Selected {:,} batches.\\n'.format(len(batch_ordered_sentences)))\n",
        "\n",
        "    # =========================\n",
        "    #        Add Padding\n",
        "    # =========================    \n",
        "\n",
        "    print('Padding out sequences within each batch...')\n",
        "\n",
        "    py_inputs = []\n",
        "    py_attn_masks = []\n",
        "    py_labels = []\n",
        "\n",
        "    # For each batch...\n",
        "    for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n",
        "\n",
        "        # New version of the batch, this time with padded sequences and now with\n",
        "        # attention masks defined.\n",
        "        batch_padded_inputs = []\n",
        "        batch_attn_masks = []\n",
        "        \n",
        "        # First, find the longest sample in the batch. \n",
        "        # Note that the sequences do currently include the special tokens!\n",
        "        max_size = max([len(sen) for sen in batch_inputs])\n",
        "\n",
        "        # For each input in this batch...\n",
        "        for sen in batch_inputs:\n",
        "            \n",
        "            # How many pad tokens do we need to add?\n",
        "            num_pads = max_size - len(sen)\n",
        "\n",
        "            # Add `num_pads` padding tokens to the end of the sequence.\n",
        "            padded_input = sen + [tokenizer.pad_token_id]*num_pads\n",
        "\n",
        "            # Define the attention mask--it's just a `1` for every real token\n",
        "            # and a `0` for every padding token.\n",
        "            attn_mask = [1] * len(sen) + [0] * num_pads\n",
        "\n",
        "            # Add the padded results to the batch.\n",
        "            batch_padded_inputs.append(padded_input)\n",
        "            batch_attn_masks.append(attn_mask)\n",
        "\n",
        "        # Our batch has been padded, so we need to save this updated batch.\n",
        "        # We also need the inputs to be PyTorch tensors, so we'll do that here.\n",
        "        # Todo - Michael's code specified \"dtype=torch.long\"\n",
        "        py_inputs.append(torch.tensor(batch_padded_inputs))\n",
        "        py_attn_masks.append(torch.tensor(batch_attn_masks))\n",
        "        py_labels.append(torch.tensor(batch_labels))\n",
        "    \n",
        "    print('  DONE.')\n",
        "\n",
        "    # Return the smart-batched dataset!\n",
        "    return (py_inputs, py_attn_masks, py_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76af739f-ad4c-4f86-d722-8907bac96d5d"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 321\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Update every `update_interval` batches.\n",
        "update_interval = 500\n",
        "# good_update_interval(total_iters=len(py_inputs), num_desired_updates=10)\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    \n",
        "    if not use_fixed_padding:\n",
        "      # At the start of each epoch (except for the first) we need to re-randomize\n",
        "      # our training data.\n",
        "      if epoch_i > 0:\n",
        "          # Use our `make_smart_batches` function to re-shuffle the \n",
        "          # dataset into new batches.\n",
        "          (py_inputs, py_attn_masks, py_labels) = make_smart_batches(train_text, train_labels, batch_size)\n",
        "    \n",
        "    print('Training on {:,} batches...'.format(len(py_inputs)))\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step in range(0, len(py_inputs)):\n",
        "\n",
        "        # Progress update every, e.g., 100 batches.\n",
        "        if step % update_interval == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Calculate the time remaining based on our progress.\n",
        "            steps_per_sec = (time.time() - t0) / step\n",
        "            remaining_sec = steps_per_sec * (len(py_inputs) - step)\n",
        "            remaining = format_time(remaining_sec)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))\n",
        "\n",
        "        # Copy the current training batch to the GPU using the `to` method.\n",
        "        b_input_ids = py_inputs[step].to(device)\n",
        "        b_input_mask = py_attn_masks[step].to(device)\n",
        "        b_labels = py_labels[step].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass.\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The call returns the loss (because we provided labels) and the \n",
        "        # \"logits\"--the model outputs prior to activation.\n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       return_dict = True)\n",
        "\n",
        "        loss = result.loss\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(py_inputs)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training on 1,563 batches...\n",
            "  Batch     500  of    1,563.    Elapsed: 0:05:32.  Remaining: 0:11:45\n",
            "  Batch   1,000  of    1,563.    Elapsed: 0:11:04.  Remaining: 0:06:14\n",
            "  Batch   1,500  of    1,563.    Elapsed: 0:16:35.  Remaining: 0:00:42\n",
            "\n",
            "  Average training loss: 0.26\n",
            "  Training epcoh took: 0:17:17\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:17:17 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5eZFCVPPn7J"
      },
      "source": [
        "# Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjeMtTILT5iU"
      },
      "source": [
        "## Load Test Dataset & Smart Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGuZAHn_S-fG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e37b818-059f-4ea8-e009-e92bf98ddb99"
      },
      "source": [
        "if use_fixed_padding:\n",
        "  (py_inputs, py_attn_masks, py_labels) = get_fixed_padding_dataset(test_text,\n",
        "                                                                    test_labels,\n",
        "                                                                    batch_size)\n",
        "else:\n",
        "  # Use our new function to completely prepare our dataset.\n",
        "  (py_inputs, py_attn_masks, py_labels) = make_smart_batches(test_text,\n",
        "                                                             test_labels,\n",
        "                                                             batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing 25,000 training samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 2,400 samples.\n",
            "  Tokenized 4,800 samples.\n",
            "  Tokenized 7,200 samples.\n",
            "  Tokenized 9,600 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "  Tokenized 14,400 samples.\n",
            "  Tokenized 16,800 samples.\n",
            "  Tokenized 19,200 samples.\n",
            "  Tokenized 21,600 samples.\n",
            "  Tokenized 24,000 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RfSDldsUmKn"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhR99IISNMg9"
      },
      "source": [
        "Apply fine-tuned model to generate predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e676b09c-1a6f-4cd1-d37e-37eac7951f21"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_labels)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Choose an interval on which to print progress updates.\n",
        "update_interval = 500\n",
        "# good_update_interval(total_iters=len(py_inputs), num_desired_updates=10)\n",
        "\n",
        "# Measure elapsed time.\n",
        "t0 = time.time()\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# For each batch of training data...\n",
        "for step in range(0, len(py_inputs)):\n",
        "\n",
        "    # Progress update every 100 batches.\n",
        "    if step % update_interval == 0 and not step == 0:\n",
        "        # Calculate elapsed time in minutes.\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        \n",
        "        # Calculate the time remaining based on our progress.\n",
        "        steps_per_sec = (time.time() - t0) / step\n",
        "        remaining_sec = steps_per_sec * (len(py_inputs) - step)\n",
        "        remaining = format_time(remaining_sec)\n",
        "\n",
        "        # Report progress.\n",
        "        print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))\n",
        "\n",
        "    # Copy the batch to the GPU.\n",
        "    b_input_ids = py_inputs[step].to(device)\n",
        "    b_input_mask = py_attn_masks[step].to(device)\n",
        "    b_labels = py_labels[step].to(device)\n",
        "  \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        result = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, return_dict=True)\n",
        "\n",
        "    logits = result.logits\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 25,000 test sentences...\n",
            "  Batch     500  of    1,563.    Elapsed: 0:01:51.  Remaining: 0:03:55\n",
            "  Batch   1,000  of    1,563.    Elapsed: 0:03:41.  Remaining: 0:02:04\n",
            "  Batch   1,500  of    1,563.    Elapsed: 0:05:32.  Remaining: 0:00:14\n",
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a16IZkJ18rs3"
      },
      "source": [
        "Calculate the test set accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li-4pvYfZbru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b22d288-cf28-45e4-8045-896aa4eb97bf"
      },
      "source": [
        "# Combine the results across the batches.\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Choose the label with the highest score as our prediction.\n",
        "preds = np.argmax(predictions, axis=1).flatten()\n",
        "\n",
        "# Calculate simple flat accuracy -- number correct over total number.\n",
        "accuracy = (preds == true_labels).mean()\n",
        "\n",
        "print('Accuracy: {:.3f}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.933\n"
          ]
        }
      ]
    }
  ]
}